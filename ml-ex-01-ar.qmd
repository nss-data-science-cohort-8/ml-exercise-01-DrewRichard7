---
title: "ML Exercise 1"
author: "Andrew Richard"
format: 
    html:
        code-fold: true
jupyter: python3
execute:
    enabled: true
---

# Machine Learning Exercise 1: Regression Basics

In this exercise, you'll be working with a dataset on house sales in King County, Washington. You can get more information about this dataset [here](https://www.kaggle.com/datasets/harlfoxem/housesalesprediction). Our first goal is to build a predictive model for the sales price.

```{python}
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV
from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso 
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, root_mean_squared_error, mean_absolute_percentage_error, confusion_matrix, classification_report, roc_curve, roc_auc_score
from sklearn.compose import TransformedTargetRegressor
```
mean squared error, root mean squared error, mean absolute error, mean absolute percentage error, and R^2 score.
We'll start with a simple model, one which uses just the square footage of living space as our only predictor variable.

1. Before building the model, start by looking at a scatterplot of price vs. the square footage of living space. What do you find? How strong does the relationship appear to be?

```{python}
house_data = pd.read_csv("./data/kc_house_data.csv")
```

```{python}
fig = plt.scatter(y = house_data['price'], x = house_data['sqft_living'])
plt.title('Price vs SQFT livingspace')
plt.ylabel('price')
plt.xlabel('sqft_living')
plt.show()
```
Now, let's work towards building the model. 

2. Create a DataFrame, X, which contains one column, the sqft_living space and a Series, y, which contains the target variable.

One of the most important concepts when it comes to machine learning is that the only performance we care about is how well the model predicts on data that it has not seen yet.

```{python}
X = house_data[['sqft_living']]
y = house_data['price']

print(X.shape, y.shape)
```
One way to accomplish this is through setting aside a portion of the data as a **test set** and to train the model on the remaining portion.

3. Use the [train_test_split function](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to create a training and a test set from X and y. Name the resulting pieces X_train, X_test, y_train, and y_test. When you do this, use 30% of the data in the test set. Also, use the random_state parameter so that we can compare model performance as we change the way we build our model.

Now, we need to fit our model. We'll start with a linear regression model. 

```{python}
X_train, X_test, y_train, y_test = train_test_split(
    X, 
    y, 
    test_size = 0.3, 
    random_state = 42
)
```
4. Import the [LinearRegression class](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) from the linear_model module. Then create a linear regression instance and fit it to the training data.

In some cases, we may want to inspect the model after it has been fit. Here, since we are working with a linear regression model, we could inspect the coefficients. 

```{python}
lm = LinearRegression()
model_fit = lm.fit(X_train, y_train)
```
5. What is the intercept term, and what is the coefficient for sqft_living? How can you interpret how the model is using sqft_living to make predictions? Hint: You can get these values by looking at the .intercept_ and .coef_ attributes of the fit model.

The next step is to evaluate how well the model does on the unseen data. 

```{python}
print(model_fit.intercept_, model_fit.coef_)
```
6. Use the predict method to generate a set of predictions on y_test (maybe X_test?). Save the results to y_pred. Question: What type of object is y_pred? What is its shape and how does it compare to y_test?

```{python}
y_pred = lm.predict(X_test)
print(type(y_pred), y_pred.shape, y_test.shape)
```
Now, we can evaluate how well the model did. There are a number of different metrics that could be used. Let's look at five of them, mean squared error, root mean squared error, mean absolute error, mean absolute percentage error, and R^2 score. For more information on these metrics, see https://developer.nvidia.com/blog/a-comprehensive-overview-of-regression-evaluation-metrics/.

7. Calculate each metric on the test set. For each one, interpret what the number means.

```{python}
print("mean squared error: ", mean_squared_error(y_test, y_pred))
print("root mean squared error: ", root_mean_squared_error(y_test, y_pred))
print("mean absolute error: ", mean_absolute_error(y_test, y_pred))
print("mean absolute percentage error: ", mean_absolute_percentage_error(y_test, y_pred))
print("R^2: ", r2_score(y_test, y_pred))
```
It's always a good idea to see how well your model performs against a simple baseline. In the case of regression, a very simple model would be one that just predicts the overall average home price on the training data.

8. Create a numpy array that is the same length as y_test which just contains a constant value equal to the mean of the target variable on the training data. Hint: you could use the [full_like function from numpy](https://numpy.org/doc/2.2/reference/generated/numpy.full_like.html) to create this array. Does the model using sqft_living do better than this simple baseline model? 

```{python}
baseline = np.full_like(y_test, np.mean(y))
print("mean squared error: ", mean_squared_error(y_test, baseline))
print("root mean squared error: ", root_mean_squared_error(y_test, baseline))
print("mean absolute error: ", mean_absolute_error(y_test, baseline))
print("mean absolute percentage error: ", mean_absolute_percentage_error(y_test, baseline))
print("R^2: ", r2_score(y_test, baseline))
```
While we started with a linear regression model, there are other types of models that we could try out. A k-nearest-neighbors model generates predictions by finding the most similar points in the training data and averaging their target values. It is a non-parametric model and doesn't assume a particular form for the relationship between the features and the target. 

9. Fit a [KNeighborsRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html) to the data using just the square footage of living space and using 5 neighbors. How do the metrics compare to those for the linear regression model? 

```{python}
knn = KNeighborsRegressor(n_neighbors = 5)
knn.fit(X_train, y_train)
knn_predict = knn.predict(X_test)
print("mean squared error: ", mean_squared_error(y_test, knn_predict))
print("root mean squared error: ", root_mean_squared_error(y_test, knn_predict))
print("mean absolute error: ", mean_absolute_error(y_test, knn_predict))
print("mean absolute percentage error: ", mean_absolute_percentage_error(y_test, knn_predict))
print("R^2: ", r2_score(y_test, knn_predict))

```

```{python}
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=42)
```
Alexander's
```{python}
neigh_model = KNeighborsRegressor(n_neighbors=5)
neigh_model.fit(X_train, y_train)
neigh_model_pred = neigh_model.predict(X_test)

print("mean squared error: ", mean_squared_error(y_test, neigh_model_pred))
print("root mean squared error: ", root_mean_squared_error(y_test, neigh_model_pred))
print("mean absolute error: ", mean_absolute_error(y_test, neigh_model_pred))
print("mean absolute percentage error: ", mean_absolute_percentage_error(y_test, neigh_model_pred))
print("R^2: ", r2_score(y_test, neigh_model_pred))
```
10. We can see how the KNN model has more flexibility than the Linear Regression model by plotting their predictions. Create a DataFrame which includes a range of typical square footage values. Hint: you can use the [numpy linspace function](https://numpy.org/doc/2.1/reference/generated/numpy.linspace.html). Generate 200 equally spaced values between 500 and 3000. Then generate predictions on these using both the linear regression and KNN model. Plot the predictions. What do you notice? 

```{python}
sqftages = np.linspace(500, 3000, 200).reshape(-1, 1)
lm_pred = lm.predict(sqftages)
kn_pred = knn.predict(sqftages)

plt.plot(lm_pred)
plt.plot(kn_pred)
```
11. Finally, let's see what happens when we add another variable. Create a new linear regression model which uses both the square footage of living space and whether or not it is waterfront. When you do this, make sure that you're using the same train and test split by using the same random_state as above. By how much does this improve the model's fit?

```{python}
X2 = house_data[["sqft_living", "waterfront"]]
X2_train, X2_test, y_train, y_test = train_test_split(X2, y, test_size = 0.3, random_state = 42)

lm2 = LinearRegression()
lm_fit_2 = lm2.fit(X2_train, y_train)
print(lm_fit_2.intercept_, model_fit.coef_)


y_pred2 = lm2.predict(X2_test)
print(type(y_pred2), y_pred2.shape, y_test.shape)

print("mean squared error: ", mean_squared_error(y_test, y_pred2))
print("root mean squared error: ", root_mean_squared_error(y_test, y_pred2))
print("mean absolute error: ", mean_absolute_error(y_test, y_pred2))
print("mean absolute percentage error: ", mean_absolute_percentage_error(y_test, y_pred2))
print("R^2: ", r2_score(y_test, y_pred2))
```
An additional feature appears to improve the model fit. 
** If you've reached this point, let your instructors know so that they can check in with you. **

Stretch Goals

1. One way to add complexity to the model is through using interaction terms. These allow for the relationship between sqft_living and price to differ for waterfront and non-waterfront properties. Create a column containing the product of sqft_living and waterfront. Add this to your model. Does it improve the performance? Interpret the meaning of the coefficients for the model containing the interaction.

**Interaction term**
```{python}
#| label: interaction
house_data['interaction'] = house_data["waterfront"] * house_data["sqft_living"]
X = house_data[["sqft_living", "waterfront", "interaction"]]
y = house_data["price"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)

lm = LinearRegression()
lm_fit = lm.fit(X_train, y_train)
print("Intercept: ", lm_fit.intercept_, "Coefs: ", lm_fit.coef_)


y_pred = lm.predict(X_test)
print(type(y_pred), y_pred.shape, y_test.shape)

print("mean squared error: ", mean_squared_error(y_test, y_pred))
print("root mean squared error: ", root_mean_squared_error(y_test, y_pred))
print("mean absolute error: ", mean_absolute_error(y_test, y_pred))
print("mean absolute percentage error: ", mean_absolute_percentage_error(y_test, y_pred))
print("R^2: ", r2_score(y_test, y_pred))

```
adding an interaction term appears to improve the model fit further


2. Take a look at the distribution of home prices. You'll find that it's highly skewed. When the target is skewed, it can be beneficial to transform the target so that it is closer to normally-distributed in order to reduce the effect of extreme values. **Important Note:** When comparing metrics for a transformed target and an untransformed target, it is important that you apply an inverse transformation to the predictions from the transformed model. This can be done using a [TransformedTargetRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.compose.TransformedTargetRegressor.html). Try using a log transformation on the target. Does this improve the metrics for the model? 

Distribution of housing prices
```{python}
plt.hist(house_data['price'], bins=20)
plt.title("housing price distribution (highly skewed)")
plt.xlabel("price")
plt.ylabel("count of homes")
```

```{python}
tt = TransformedTargetRegressor(regressor=LinearRegression(),
func=np.log, inverse_func=np.exp)

tt_fit = tt.fit(X_train, y_train)
tt_pred = tt.predict(X_test)

plt.hist(tt_pred, bins = 20)

print("Coefs: ", tt.regressor_.coef_)
print("mean squared error: ", mean_squared_error(y_test, tt_pred))
print("root mean squared error: ", root_mean_squared_error(y_test, tt_pred))
print("mean absolute error: ", mean_absolute_error(y_test, tt_pred))
print("mean absolute percentage error: ", mean_absolute_percentage_error(y_test, tt_pred))
print("R^2: ", r2_score(y_test, tt_pred))
```
